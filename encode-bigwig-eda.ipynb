{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75cf24ae-65a5-444d-a1c0-cf9ea699ee12",
   "metadata": {},
   "source": [
    "# ENCODE bigWig analysis questions\n",
    "\n",
    "Do your best to answer all parts of each question. You are encouraged to work in pairs. \n",
    "\n",
    "Answers to many questions may benefit from using more than one cell, as well as a combination of markdown and code cells.\n",
    "\n",
    "Put helper functions into a separate script (e.g. `hwutils.py`) so they can be commented on easily and focus the notebook on plotting. Also see the [workshop on Clean Code](https://drive.google.com/file/d/1TraVwRkbkCbHq-s_-NS69ZEbRNwH8XNh/view) from Dan Larremore (https://larremorelab.github.io/slides/) for good coding tips to use in this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c197bf3-c444-4a35-ace2-166bdbb117e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful libraries to import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import  sklearn.decomposition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import EngFormatter\n",
    "bp_formatter = EngFormatter('b') \n",
    "# nice way to format ticks as human-readable: ax.xaxis.set_major_formatter(bp_formatter)\n",
    "\n",
    "from hwutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546f1f82-3fc4-41f2-8bac-ac946f303874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16521/3656987943.py:7: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bigwig_metadata = pd.read_table('/mnt/g/My Drive/Academics/QBIO_577/Week_8/qbio577_fall2022/data/ENCODE_GRCh38_bigWig_metadata.tsv')\n"
     ]
    }
   ],
   "source": [
    "# load dataFrame of bigWigs from ENCODE (encodeproject.org/), binned to 10kb resolution across chromosome 10.\n",
    "# note that the first three columns are chrom,start,end and the other columns are labeled by bigWig file accession.\n",
    "df = pd.read_table('/mnt/g/My Drive/Academics/QBIO_577/Week_8/qbio577_fall2022/data/ENCODE_GRCh38_binned_subset.tsv')\n",
    "\n",
    "# load metadata from ENCODE for bigwig files. \n",
    "# can be queried as follows: bigwig_metadata.query(\"`File accession`==@ df_column_name \")\n",
    "bigwig_metadata = pd.read_table('/mnt/g/My Drive/Academics/QBIO_577/Week_8/qbio577_fall2022/data/ENCODE_GRCh38_bigWig_metadata.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218a48d-de2d-49ee-8565-5a71bef34e2f",
   "metadata": {},
   "source": [
    "- After loading the data (above), and visualize some of the profiles. Why might many signals dip on chr10 at around 40Mb?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b87fa2-cc31-408f-9b89-53a9cc9b00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90a9a1a3-9d9b-4795-900e-5b9ed585d2a1",
   "metadata": {},
   "source": [
    "- Use scikit-learn to perform PCA, and make a scatterplot of PC1 vs PC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d8003-9bb8-4d8a-aec9-2435cf72ffff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86b8e8-9062-4e09-88de-6cea9f968da3",
   "metadata": {},
   "source": [
    "- Try to use the experiment metadata to understand and remove outliers. Try labeling or coloring points by various metadata columns. Were any columns in the metadata useful for outlier removal? Note that `sklearn.preprocessing.LabelEncoder()` can be useful for transforming text strings to categories, and `plt.text` can be used to overlay labels with points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eec396-279a-4379-b580-e4c5dd591a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db5d9d2-be7e-47ea-9861-695620fe7db3",
   "metadata": {},
   "source": [
    "- Which Assays or Experiment Targets show broad vs narrow patterns? Is this consistent across cell types? Does this relate to the patterns seen in PCA? One way to investigate the characteristic scale is by computing the autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c2e33-6bcb-427e-be2d-9d476d5e2c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1b80847-fcff-4d7e-a709-ffbe494521b5",
   "metadata": {},
   "source": [
    "- Which \"Experiment Targets\" (e.g. histone marks or transcription factors) for which cell types are nearby in this PC1 vs PC2 space? Do any of these proximities have plausible biological interpretations? For example, are any polycomb-related factors in proximity? Illustrate this in a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd295e-998a-4431-8216-dd9337e07c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20133d7e-2648-4683-8c1a-b5a27c07f381",
   "metadata": {},
   "source": [
    "- How much does preprocessing matter? Try normalizing the variance per track and see if you arrive at similar or distinct conclusions. Try removing the region on chr10 mentioned above. Note that `sklearn.preprocessing.StandardScaler` could be useful for preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cd712-6b53-4068-966e-47b69700e739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0725a826-9671-4bd4-b864-c4974864f85a",
   "metadata": {},
   "source": [
    "- How many PCs are needed to explain 90% of the variance in the data? Illustrate this with a scree plot (https://en.wikipedia.org/wiki/Scree_plot). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fc426-688f-420e-8716-cdc126e76075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847204dc-d85a-4ed4-b850-7fc8bbf4e80f",
   "metadata": {},
   "source": [
    "- How different is the dimensionality reduction into two dimensions for PCA from that obtained using MDS (multi-dimensional scaling)? What methods could be used to determine the similarity? Illustrate with a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b0493-14a5-4d40-beba-a3c53e60f5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d37b2a46-fdeb-42e3-9f9a-53104261bc12",
   "metadata": {},
   "source": [
    "- Would non-negative matrix factorization (https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) be a useful method to use for this dataset? Why or why not?  (No plots needed for this question).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce8373-2e5a-4644-836a-d06d9686481c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5887580c7cbb0fe8c117cf89484cb0db59155905523768f16ba2f70bd8e79c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
